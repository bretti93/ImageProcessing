---
title: 'HarvardX - PH125.9x - DataScience: Capstone - ImageProcessing'
author: "Jan Brettschneider"
date: "15th April 2020"
output: pdf_document
---

## 1. Introduction
This document summarizes the chosen project "Image Processing" for the Capstone Exam in course of the HardvardX Data Science Certificate.

The dataset used is the **Fashion-MNIST** dataset published by the company Zalando. It contains 28x28 pixel grayscale images of the articles which are associated with 10 classes. It is commonly used for benchmarking machine learning algorithms. Due to the fact that we already used the classical MNIST dataset with numbers, I'm really interested in the processing of pictures and this dataset is already tidy and proven, I decided to give it a go.

This project again will have the classical approach of machine learning projects, although I will put the focus on the machine learning part of things. The steps performed are as follows:

  1. Download the raw data and import the files into R Studio
  2. Process and explore the data, to find out more how to proceed during model creation
  3. Build and optimize different models
  4. Use the best model on the test set and evaluate the results
  5. Conclution, limitations and comments
  
  
The ambition is to get a better understanding of which parameters and methods can be used for image processing problems.


### The dataset
The original dataset can be found in different sources. I downloaded the data from **[Kaggle](https://www.kaggle.com/zalando-research/fashionmnist)** and reuploaded it into my dropbox so it can be automatically downloaded without having to register on the website.

In addition to the packages used during the MovieLens project I utilized the **downloader package**. The code for downloading and extracting the model looks as follows:
```{r download_data, message=FALSE, warning=FALSE}
################################
# Create train set, test set
################################

# Load required packages if required
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(downloader)) install.packages("downloader", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")

# Download the dataset from my Dropbox and unzip it
url <- "https://www.dropbox.com/s/7yzdmv2im26hauh/fashionmnist.zip?raw=1"
download(url, dest="fashionmnist.zip", mode="wb") 
unzip ("fashionmnist.zip", exdir = ".")
```

Afterwards we are loading the .csv files into R and storing them into our variables:
```{r import_data, message=FALSE, warning=FALSE}
# Create the data set for model building and verification
verification = read.csv("fashion-mnist_test.csv", header = TRUE)
modelling = read.csv("fashion-mnist_train.csv", header = TRUE)
```

Now two datasets have been created one for modelling with 60,000 samples and one for the testing/verification of our final model with 10,000 samples.
Let's take a look at the structure of the data:
```{r data_sample, warning=FALSE}
modelling[1:5,1:10]
```

Each row contains the label in the first column and greyvalue for the 28 by 28 pixels of the image stored in the columns 2 to 785. Formulated differently, we have one picture plus label stored in each row of the matrix.

For easier processing we are also seperating the labels from the images:
```{r seperate_labels, message=FALSE, warning=FALSE}
# Seperate labels and images
x <- modelling[,2:ncol(modelling)]
y <- factor(modelling[,1])
x_verification <- verification[,2:ncol(verification)]
y_verification <- factor(verification[,1])
```

The images are currently stored in a matrix as you can see here:
```{r matrix_structure, warning=FALSE}
dim(x)
dim(x_verification)
```

For some kind of analysis it is easier to have arrays, so we create an additional dataset with reshaped data:
```{r create_arrays, message=FALSE, warning=FALSE}
x_images <- array(as.numeric(unlist(x)), dim=c(60000, 28, 28))
x_verification_images <- array(as.numeric(unlist(x_verification)),
                               dim=c(10000, 28, 28))
```

Now we have the data in the following format:
```{r array_structure, warning=FALSE}
dim(x_images)
dim(x_verification_images)
```

Now we have to split the dataset into training-set and test-set to train our models.
```{r create_subsets, message=FALSE, warning=FALSE}
x_mod <- modelling[1:50000,]
x_test <- modelling[50001:60000,]
x_sub <- x_images[1:50000,,]
y_sub <- y[1:50000]
x_test_sub <- x_images[50001:60000,,]
y_test_sub <- y[50001:60000]
```

And as a last step in this section we will create the fashion categories according to the numbers which indicate the categories:
```{r create_categories, message=FALSE, warning=FALSE}
fashion_cat = c('T-shirt/top',
                'Trouser',
                'Pullover',
                'Dress',
                'Coat', 
                'Sandal',
                'Shirt',
                'Sneaker',
                'Bag',
                'Ankle boot')  
```


## 2. Analysing the data
In this section we will explore the data more in depth. How do the images acutally look like, do we see patterns between the categories or do we have some promising points for leveraging the most out of our method toolbox?

Let's take a look at the first 32 pictures of the dataset:

```{r plot_pictures_code, message=FALSE, warning=FALSE}
# Plot the first 16 images with labels to get an impression how they look
# Define a four by four grid for the plot
par(mfrow=c(4,4))
# Setting the margins around the pictures with "mar" 
par(mar=c(0, 0, 1.5, 0))

# Plot every image (remember to reverse) on a gray scale with label
# Reverse the color scheme to get a white background
for (i in 1:16) {
  image(1:28, 1:28, as.matrix(x_images[i,,])[,28:1], col = gray((255:0)/255), xaxt = 'n',
        yaxt = 'n', xlab = "", ylab = "", asp = 1, main = paste(fashion_cat[y[i]]))
}

# Plot the next 16 images with the same logic
for (i in 17:32) {
  image(1:28, 1:28, as.matrix(x_images[i,,])[,28:1], col = gray((255:0)/255), xaxt = 'n',
        yaxt = 'n', xlab = "", ylab = "", asp = 1, main = paste(fashion_cat[y[i]]))
}
```

As we can see, we have some "low quality" pictures of several types of fashion, which can be relatively easily identified manually.
But how can we do that with help of machine learning?

Let us start simple. First let us check whether we can set a threshold for making the pictures even simpler:
```{r intensity_distribution, warning=FALSE}
qplot(as.vector(x_images), bins = 30, color = I("black"))
```

As we can see there is no clear cutoff, but we can try to set a low cutoff threshold, so we only have black and white left without anything in between.
Here is the code we use on the subset (faster computation) for a cutoff below 10:
```{r create_blackwhite , message=FALSE, warning=FALSE}
x_sub_bw <- ifelse(x_sub < 10, 0 , 1)
```

Here are the same 32 pictures again
```{r plot_pictures_bw, message=FALSE, warning=FALSE}
# Plot the first 16 images with labels to get an impression how they look
# Define a four by four grid for the plot
par(mfrow=c(4,4))
# Setting the margins around the pictures with "mar" 
par(mar=c(0, 0, 1.5, 0))

# Plot every image (remember to reverse) on a gray scale with label
# Reverse the color scheme to get a white background
for (i in 1:16) {
  image(1:28, 1:28, as.matrix(x_sub_bw[i,,])[,28:1], col = gray(1:0), xaxt = 'n',
        yaxt = 'n', xlab = "", ylab = "", asp = 1, main = paste(fashion_cat[y[i]]))
}

# Plot the next 16 images with the same logic
for (i in 17:32) {
  image(1:28, 1:28, as.matrix(x_sub_bw[i,,])[,28:1], col = gray(1:0), xaxt = 'n',
        yaxt = 'n', xlab = "", ylab = "", asp = 1, main = paste(fashion_cat[y[i]]))
}
```

As we can see, this does not help us. for example picture 3, 6 and 15 (shirt, coat and pullover) now look even more similar. So we will not pursue this approach any further.

Next, we can take a look at the intensity of the pictures, so the mean graycolor value for each picture. We exclude the first column, which contains the label, group by the label and add the fashion category vector so we have a better understanding:
```{r summarize_intensity, warning=FALSE}
arrange(cbind(modelling %>% mutate(row_mean = rowMeans(modelling[,-1])) %>%
        group_by(label) %>% summarize(avg_pic_int = mean(row_mean)),fashion_cat),
        desc(avg_pic_int))
```

Here is the standard deviation for the intensity by category:
```{r summarize_sds, warning=FALSE}
arrange(cbind(modelling %>% mutate(row_sds = rowSds(as.matrix(modelling[,-1]))) %>%
        group_by(label) %>% summarize(avg_sds = mean(row_sds)),fashion_cat),
        desc(avg_sds))
```

Furthermore we look how many pixels are unequal to zero, and therefore used for the actual image excluding the background:
```{r summarize_pixels, warning=FALSE}
arrange(cbind(modelling %>% mutate(pixels = rowSums(modelling[,-1]>0)) %>%
        group_by(label) %>% summarize(avg_pixels_used = mean(pixels)),fashion_cat),
        desc(avg_pixels_used))
```


We can also calculate the average pixel intensity by dividing the sum of the grayscale values divided by the number of pixels unequal to zero:
```{r summarize_pixel_intensity, warning=FALSE}
arrange(cbind(modelling %>% mutate(pixel_int = rowSums(modelling[,-1])/
        rowSums(modelling[,-1]>0)) %>% group_by(label) %>%
        summarize(avg_pixel_int = mean(pixel_int)),fashion_cat),
        desc(avg_pixel_int))
        
```

Finally we take a look at the variability by pixel on overall level. Herefore we calculate and plot the standard deviation by pixel and make it even more understandable by visualizing the results. The code looks as follows:
```{r sds_by_pixel, warning=FALSE}
sds_image <- colSds(as.matrix(x))
qplot(sds_image, bins = "30", color = I("black"))
image(1:28, 1:28, matrix(sds_image, 28, 28)[,28:1], asp = 1, main = "Sds by pixel")
```

So what can we conclude from the analysis? We can see that we can't set a threshold for the grayscale value to enhance the possibility for recognizing images, because the shapes in some categories are to similar.

What we can see is that we have a significant difference when looking at the color intensity for the images. Coats, pullovers and bags have much more "average grey-intensity" than trousers, sneakers and sandals.
Ankle boats and coats have a high standard deviations, whereas shirts, sneakers and sandals significantly lower ones.

These metrics do not take the actual "pixels used" into consideration. Pullovers have the highest average non-black pixels, double as much as sandals. The intensity for the non-black pixels could also be an interesting aspect, because they create seperation in another area.

We can use this "parameter based" approach first for build models and compare the results to other methodes which seem to be more suited for the given task like knn.

## 3. Modelling
As we discussed in the earlier section, we will test different methods on the very basic approach of taking the caluclated metrics for setting up a model.